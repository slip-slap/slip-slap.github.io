---
layout: post
title: "Model: Gaussian Mixture Model"
keywords: ["Kmeans", "GMM"]
description: ""
category: "AI"
tags: ["unsupervised learning" ]
---
{% include JB/setup %}

#### One Dimensional GMM
<img src="{{IMAGE_PATH}}/AI-model-gmm.png">
The question is how to understand the coefficient between each $x_i$
1. suppose a point value is 10, and the coefficient is very small which is
   0.02, you can understand this in this way, this contribution of this point is
   kind of pull the Gauss distribution to the left, becasue its contribution to
   the mean value of Gauss is 0.2. It is kind of drag the distribution to the
   left part of the coordinate.
2. How to understand the coeffient pops up in the denominator, You can treat it
   in this way, if you don't have the the coefficient, what should be as
   denominator.

#### model 

we are assuming that these data are Gaussian and we want to maximize the likelihood of 
observing these data.The following equation tells us that a particular point **x** is a
**linear combination of the K Gaussians.**


$$
\begin{array}{c}{p(x)=\sum_{j=1}^{k} \phi_{j} \mathcal{N}\left(x ; \mu_{j}, \Sigma_{j}\right)} \\ {\sum_{j=1}^{k} \phi_{j}=1}\end{array}
$$


#### Expectation-Maximization

At the Expectation step, we compute a matrix where rows are the data points and the column are Gaussians.
An element at row **i**, column **j** is that the probability that $$x^{i}$$ was generated by the Gaussian **j**
The denominator just sums over all values to make **W** a probability

$$
W_{j}^{(i)}=\frac{\phi_{j} \mathcal{N}\left(x^{(i)} ; \mu_{j}, \Sigma_{j}\right)}{\sum_{q=1}^{k} \phi_{q} \mathcal{N}\left(x^{(i)} ; \mu_{q}, \Sigma_{q}\right)}
$$

1. The first equation is just the sum of a particular Gaussian **j** divided by the number of points
2. In the second equation, we just computing the mean, except we multiply by the probabilities for that cluster
3. In the last equation, we are just computing the covariance, except we multiply by the probabilities for that cluster



$$
\begin{aligned} \phi_{j} &=\frac{1}{N} \sum_{i=1}^{N} W_{j}^{(i)} \\ \mu_{j} &=\frac{\sum_{i=1}^{N} W_{j}^{(i)} x^{(i)}}{\sum_{i=1}^{N} W_{j}^{(i)}} \\ \Sigma_{j} &=\frac{\sum_{i=1}^{N} W_{j}^{(i)}\left(x^{(i)}-\mu_{j}\right)\left(x^{(i)}-\mu_{j}\right)^{T}}{\sum_{i=1}^{N} W_{j}^{(i)}} \end{aligned}
$$



| GMM | Cluster 1 | Cluster 2 | Cluster 3 |
|:------:|:---------:|:---------:|-----------|
| data 1 |   $$w_{1}^{1}$$   |   $$w_{2}^{1}$$     |  $$w_{3}^{1}$$       |
| data 2 |   $$w_{1}^{2}$$   |   $$w_{2}^{2}$$     |  $$w_{3}^{2}$$      |
| data 3 |   $$w_{1}^{3}$$   |   $$w_{2}^{3}$$     |  $$w_{3}^{3}$$      |
| data 4 |   $$w_{1}^{4}$$   |   $$w_{2}^{4}$$     |  $$w_{3}^{4}$$     |


#### Galois
1. If you don't understand in two dimensional GMM, maybe you should start with
   one dimensional GMM.
2. There is one question left, how to understand the coefficient when we
   calculate the mean, covaiance.


#### Reference
[Clustering with Gaussian Mixture Model](https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/)

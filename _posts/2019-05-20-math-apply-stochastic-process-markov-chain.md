---
layout: post
title: "P.A.S.: Markov Chain"
keywords: ["Markov Chain", "directed graph"]
description: ""
category: "math"
tags: ["apply"]
---
{% include JB/setup %}

+ Markov Chain is just a fancy term for a random walk on a graph
+ what it says it that for a very long walk, the probability that you end at some vertex *V* is independent of where you started

<img src="{{IMAGE_PATH}}/Markov-Chain.png" height="" width="" />

#### definition
Ergodic Markov Chains: A Markov Chain is ergodic if there is a positive probability to pass from any state to any other states in one step.

---
layout: post
title: "M.C.: Markov Chain and Random Walk"
keywords: []
description: ""
category: "math"
tags: ["apply","S.P.","M.C."]
---
{% include JB/setup %}

+ Markov Chain is just a fancy term for a random walk on a graph
+ what it says it that for a very long walk, the probability that you end at
  some vertex *V* is independent of where you started

<img src="{{IMAGE_PATH}}/Markov-Chain.png" height="" width="" />

#### definition
Ergodic Markov Chains: A Markov Chain is ergodic if there is a positive probability to pass from any state to any other states in one step.

---
layout: post
title: "I.T.: Idea"
keywords: [""]
description: ""
category: "math"
tags: ["apply","I.T."]
---
{% include JB/setup %}

#### Inspiration
1. A basic idea in information theory is that information can be treated very
   much like a physical quantity, such as mass or quantity.

- units: information is measured in bits, and one bit of information allows you
  to choose between two equally probable


#### Information

$\textbf{Shannon information} = log \frac{1}{p(x_h)} \textbf{bits}$ 




#### Bits Are Not Binary Digits
The word bit is derived from binary digit, but a bit and a binary digit are
fundamentally different types of quantities.
- A binary digit is the value of a binary variable
- a bit is an amount of information

#### Galois
1. Entropy is an elusive concept, how to interpret it? The natural can be
   treated as a process of entropy increasing, which means from order to
   disorder, disorder means uniform, everything is uniform distributed.
2. As in probability theory, if things are uniform distributed, which means
   things are completely chaos, uniform distributed. So the entropy is biggest.

3. Communitcation over a noisy channel leads entropy increase, which makes
   things more disorder. Because process as entropy increasing, that's
   universal, so communication over a noisy channel definitely increase it's
   entropy.

4. Before shannon, the entropy is an idea, but not expressed in a mathematic way.


#### Reference
1. [information and
   entropy](http://users.fred.net/tds/lab/information.is.not.uncertainty.html)
2. [A better description of entropy](https://www.youtube.com/watch?v=w2iTCm0xpDc)


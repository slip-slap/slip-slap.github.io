I"
<h4 id="inspiration">Inspiration</h4>
<ol>
  <li>A basic idea in information theory is that information can be treated very
much like a physical quantity, such as mass or quantity.</li>
</ol>

<ul>
  <li>units: information is measured in bits, and one bit of information allows you
to choose between two equally probable</li>
</ul>

<h4 id="information">Information</h4>

<p>$\textbf{Shannon information} = log \frac{1}{p(x_h)} \textbf{bits}$</p>

<h4 id="bits-are-not-binary-digits">Bits Are Not Binary Digits</h4>
<p>The word bit is derived from binary digit, but a bit and a binary digit are
fundamentally different types of quantities.</p>
<ul>
  <li>A binary digit is the value of a binary variable</li>
  <li>a bit is an amount of information</li>
</ul>

<h4 id="galois">Galois</h4>
<ol>
  <li>Entropy is an elusive concept, how to interpret it? The natural can be
treated as a process of entropy increasing, which means from order to
disorder, disorder means uniform, everything is uniform distributed.</li>
  <li>
    <p>As in probability theory, if things are uniform distributed, which means
things are completely chaos, uniform distributed. So the entropy is biggest.</p>
  </li>
  <li>
    <p>Communitcation over a noisy channel leads entropy increase, which makes
things more disorder. Because process as entropy increasing, that’s
universal, so communication over a noisy channel definitely increase it’s
entropy.</p>
  </li>
  <li>Before shannon, the entropy is an idea, but not expressed in a mathematic way.</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="http://users.fred.net/tds/lab/information.is.not.uncertainty.html">information and
entropy</a></li>
  <li><a href="https://www.youtube.com/watch?v=w2iTCm0xpDc">A better description of entropy</a></li>
</ol>

:ET
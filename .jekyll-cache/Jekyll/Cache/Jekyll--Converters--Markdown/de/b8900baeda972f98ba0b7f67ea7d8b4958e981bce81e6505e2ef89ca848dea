I"„
<h4 id="occams-razor">Occamâ€™s Razor</h4>
<p><img src="/images/computer-science-model-comparsion.png" /></p>
<ol>
  <li>how many boxes are behind the tree ? Occamâ€™s razor advises buy the simplest.
    <ul>
      <li>A theory with mathematical beauty is more likely to be correct than an ugly
one that fits some experimental data</li>
    </ul>
  </li>
</ol>

<h4 id="bayes-theorem-for-two-levels-of-inference">Bayesâ€™ Theorem For two levels of inference</h4>
<ol>
  <li><strong>Model Fitting</strong>: At the first level of inference, we assume that one model,
the ith, say is true, and we infer waht the models parameter <strong>w</strong> might be,
given the data D.
\(P\left(\mathbf{w} | D, \mathcal{H}_{i}\right)=\frac{P\left(D | \mathbf{w},
\mathcal{H}_{i}\right) P\left(\mathbf{w} | \mathcal{H}_{i}\right)}{P\left(D |
\mathcal{H}_{i}\right)}\)
    <ul>
      <li>The normalizing constant is commonly ignored since it
is irrelevant to the first level of inference, but it becomes important in the
second level of inferences, and we name it the evidence for 
$\mathcal{H}_{i}$</li>
    </ul>
  </li>
  <li><strong>Model Comparsion</strong>: At the second level of inference, we wish to infer
which model is most plausible given the data.
    <ul>
      <li>Model comparison is a difficult task because it is not possible simply to
choose the model that fits the data best:</li>
    </ul>
  </li>
</ol>

:ET
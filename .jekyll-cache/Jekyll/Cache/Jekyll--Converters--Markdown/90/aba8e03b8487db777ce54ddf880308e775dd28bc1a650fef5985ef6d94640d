I"£	
<h4 id="motivation">Motivation</h4>
<p>Why should $log{\frac{1}{p_i}}$ have anything to do with the information
content? why not some other function of $p_i$?</p>

<h4 id="haha">HAHA</h4>
<ol>
  <li>The Shannon information content can be intimately conected to the size of a
file that encodes the outcomes of a random experiment.</li>
</ol>

<h4 id="compressor">Compressor</h4>
<p>There are only two ways in which a â€˜compressorâ€™ can actually compress files:</p>
<ol>
  <li>A lossy compressor compresses some files, but maps some files to the same
encoding.</li>
  <li>A lossless compressor maps all files to different encodings; if it shortens
some files, <strong>it necessarily makes other longer</strong>.</li>
</ol>

<p>The smallest $\delta$-sufficient subset $S_{\delta}$ is the smallest subset of
$\mathcal{A}_X$ satisfying <br /></p>

<p>\(P\left(x \in S_{\delta}\right) \geq 1-\delta\)</p>
<ul>
  <li>
    <p>for each value of $\delta$ we can then define a new measure of information
content, the log of the size of this smallest subset $S_{\delta}$</p>
  </li>
  <li>
    <p>The essential bit content of X is: <br />
\(H_{\delta}(X)=\log _{2}\left|S_{\delta}\right|\)</p>
  </li>
</ul>

<h4 id="simple-data-compression-methods">Simple Data Compression Methods</h4>
<ol>
  <li>One way of measuring the information content of a random variable is simply
to count the number o possible outcomes, $|\mathcal{A}_X|$. (The number of
elements in a set $\mathcal{A}$ is denoted by $|\mathcal{A}|$)</li>
</ol>

<h4 id="shannons-source-coding-theorem">Shannonâ€™s Source Coding Theorem</h4>
<p>N i.i.d. random variables each with entropy H(X) can be compressed into more
than NH(X) bits with negligible risk of information, as $N \rightarrow \infy$;
conversely if they are compressed into fewer than NH(X) bits it is virtually
certain that information will be lost.</p>

<h4 id="asymptotic-equipartition-principle">Asymptotic Equipartition Principle</h4>
<p>For an ensemble of N independent identically distributed random variables $X^N
\equiv (X_1,X_2,\cdots,X_N)$, with N sufficiently large, the outcome
$x=(x_1,x_2,\cdots,x_N)$ is almost certain to belong to a subset of $\mathcal{A}^N_X$ having
only $2^{NH(X)}$ members, each having probability close to $2^{-NH(X)}$</p>
<ul>
  <li>denote the ensemble $x=(x_1,x_2,\cdots,x_N)$ by $X^N$</li>
</ul>

<h4 id="galois">Galois</h4>
<ol>
  <li>Source coding just another terminology for data compression.</li>
</ol>

:ET
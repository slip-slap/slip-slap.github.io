I"	
<h4 id="a-simple-linear-regression">a simple linear regression</h4>
<p>Let’s look at a basic linear regression. we want to map input into real numbers.
for example,</p>
<ol>
  <li>the input variable is <strong>how many hours of studying and the number of subject taken</strong></li>
  <li>the target is <strong>GPA</strong></li>
</ol>

<p>given the observation data(x,\(\hat{y}\)), and x has two components, which is the number of hours of 
studying \(x_{1}\) and the number of subject \(x_{2}\) <br /></p>

<p>for the linear regression, the target is:  <br /></p>

\[L=\frac{1}{2}\|\hat{y}-y\|_{2}^{2}+\frac{\lambda}{2}\|W\|_{2}^{2}\]

<p>y is given by \(y=W^{T} x\)</p>

<p>the simplest method to get W value is differentiate the target function. But now we get solve the problem
in a different view.</p>
<h4 id="solve-the-problem-in-bayesian-way">solve the problem in bayesian way</h4>

<ol>
  <li>second we assume the data \(y \sim N\left(\mu, \sigma^{2}\right)
\text{with  }
\mu=y=W^{T} x\)</li>
</ol>

\[P(\hat{y} | x, W)=N\left(\hat{y} | W^{T} x, \sigma^{2}\right)\]

<ol>
  <li>According to the bayes formula, 
\(P(W | \hat{y}, x)=\frac{P(\hat{y} | x, W) P\left(W | \mu_{0}, \sigma_{0}^{2}\right)}
{\int_{W} P\left(y^{\prime} | x^{\prime}, W\right) P(W | \hat{y}, x)}\)</li>
</ol>

<p>now, consider the denominator,how many possible values of W,W contains three independent parameters,
and each parameter Range is R, so how to calculate the posterior probability?
<strong>how how how</strong></p>

<ol>
  <li>first we assume the parameter</li>
</ol>

\[W=[a_{1},a_{2},b_{0}]\]

<p>, we can assume</p>

\[a1 \sim N\left(\mu_{a1}, \sigma_{a1}^{2}\right)\]

\[a2 \sim N\left(\mu_{a2}, \sigma_{a2}^{2}\right)\]

\[b0 \sim N\left(\mu_{b0}, \sigma_{b0}^{2}\right)\]

<h4 id="the-relationship-between-mle-and-map">the relationship between MLE and MAP</h4>
<p>MLE is a special case of MAP, which prior probability all same</p>

<h4 id="galois">Galois</h4>
<ol>
  <li>variation: something that is similar to something else but different in some
way. (variational)</li>
  <li>In Chinese, we name this method “变分EM”, who can understand this crap.</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://wiseodd.github.io/techblog/2017/01/01/mle-vs-map/">MLE and MAP</a></li>
</ol>
:ET
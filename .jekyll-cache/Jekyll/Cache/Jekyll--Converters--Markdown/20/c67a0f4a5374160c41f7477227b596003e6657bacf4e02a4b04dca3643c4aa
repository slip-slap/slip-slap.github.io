I"É

<h4 id="example">Example</h4>
<p>Jo has a test for a disease, we denote Joâ€™s state of health by the variable a
and the test result by b. <br /></p>
<ol>
  <li>a=1 Jo has the disease</li>
  <li>a=0 Jo does not have the disease</li>
</ol>

<p>The essential difference between frequentist and bayesian statisticians is in</p>

<p>The Bayesâ€™s theorem:</p>

\[P(\theta | x)=\frac{P(x | \theta) P(\theta)}{P(x)}\]

<ul>
  <li>The thing P(\(\theta\)|x) on the left is posterior distribution</li>
  <li>The thing P(\(\theta\)) on the right is prior distribution which presents our pre-existing beliefs.</li>
  <li>The thing P(x|\(\theta\)) on the right is likelihood which is depend on our modal and data</li>
  <li>The thing P(x) on the right is marginal probability which normalises the distribution
There are three different condition
    <ol>
      <li>most simple, both of X and Y are discrete</li>
      <li>most difficult, both of X and Y are continuous</li>
    </ol>
  </li>
</ul>

<h4 id="example-1">Example</h4>
<p>if you are a frequentist, you will not believe the crap above, letâ€™s look at an example:</p>

<p>if the weight of a dog is normal distribution, and we have some data about the dog when we weighted 
the dog at different time. we got the data is (15.3, 14, 17). and now what are the parameters of the normal
distribution ?</p>

<p>if you are a bayesian,</p>
<ul>
  <li>first, you have a prior distribution which maybe data \(X \sim N(15,2)\), maybe itâ€™s not right, but I donâ€™t care.
As you can see, we believe the parameters of \(\mu\), itâ€™s not a constant, itâ€™s distributed with \(\mu\) equals 15,and \(\sigma\) equals \(\sqrt{2}\)</li>
  <li>second, you have data, you calculate the probablitity of \(P(15.3 | 15,2)\) \(P(14 | 15,2)\) \(P(17 | 15,2)\).
and now you get the posterior distribution about the parameter of \(\mu\)</li>
  <li>finally, according to the posterior distribution of \(\mu\), you could make a decision about the parameter of \(\mu\)</li>
</ul>

<p>if you are a frequentist,</p>
<ul>
  <li>first, you believe the \(\mu\) and \(\sigma\) of the data distribution are constants</li>
  <li>second, according to center limit theroem, you will use the likelihood function and calculate the stationary value of the likelihood function.</li>
  <li>finally, you get the constants about the \(\mu\) and \(\sigma\)</li>
</ul>

<h4 id="bayesian">Bayesian</h4>
<ol>
  <li>You cannot do inference without making assumptions.</li>
  <li>The <strong>big difference</strong> between the two approaches is that Bayesians also use
probabilities to <strong>describe inferences</strong>.</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">bayesian</a></li>
</ol>

:ET
I"U
<h4 id="haha">HAHA</h4>
<p>Source Coding Theorem(Symbol Codes): There exists a variable-length encoding C
of an ensemble X such that the average length of an encoded symbol,
L(C,X),satisfies $L(C,X) \in [H(X), H(X)+1) $</p>

<h4 id="notation-for-alphabets">Notation for alphabets.</h4>
<ol>
  <li>$\mathcal{A}^N$ denotes the set of ordered N-tuples of element from the set
$\mathcal{A}$, i.e., all strings of length N.
    <ul>
      <li>
\[\{0,1\}^{3}=\{000,001,010,011,100,101,110,111\}\]
      </li>
    </ul>
  </li>
  <li>The symbols $\mathcal{A}^+$ will denote the set of all strings of finite
length composed of elements from the set $\mathcal{A}$
    <ul>
      <li>
\[\{0,1\}^{+}=\{0,1,00,01,10,11,000,001, \ldots\}\]
      </li>
    </ul>
  </li>
</ol>

<h4 id="definition">Definition</h4>
<ol>
  <li>Prefix Code: A symbol code is called a prefix code if no codeword is a prefix of any other
codeword.</li>
  <li>Expected Length: expected length $L(C,X)$ of a symbol code C for ensemble X
is  <br />
\(L(C, X)=\sum_{x \in \mathcal{A}_{X}} P(x) l(x)\)</li>
</ol>

<h4 id="kraft-inequality">Kraft inequality</h4>
<p>For any uniquely decodeable code $C(X)$ over the binary alphabet ${0,1}$, the
codeword lengths must satisfy: <br />
\(\sum_{i=1}^{I} 2^{-l_{i}} \leq 1\) <br />
where $I=|\mathcal{A}_X|$</p>
<ul>
  <li>Completeness. If a uniquely decodeable code satisfies the Kraft inequality
with equality then it is called a complete code.</li>
</ul>

<h4 id="huffman-coding-algorithm">Huffman Coding Algorithm</h4>
<ol>
  <li>Take the two least probablie symbols in the alphabet. These two symbols will
be given the longest codewords, which will have equal length, and differ only
in the last digit.</li>
  <li>Combine these two symbols into a single symbol, and repeat.</li>
</ol>

<h5 id="disadvantages-of-the-huffman-code">Disadvantages of the Huffman code</h5>

:ET
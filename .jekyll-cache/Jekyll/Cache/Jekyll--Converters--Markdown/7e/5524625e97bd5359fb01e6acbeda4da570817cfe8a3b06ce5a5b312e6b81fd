I"Õ
<h4 id="background">Background</h4>
<p>In 1960 Widrow introduced the ADALINE(ADAptive LInear NEuron) network, and a
learning rule which called the LMS(Least Mean Square) algorithm.</p>

<h4 id="drawback">Drawback</h4>
<ol>
  <li>lack of success in adapting the algorithm to multiplayer networks</li>
  <li>Widrow stopped work on neural networks in the early 1960s and began to work
full time on adaptive signal processing.</li>
</ol>

<h4 id="with-perctpron">With Perctpron</h4>
<ol>
  <li>Both the ADALINE and the perceptron suffer from the same inherent limitation,
they can only solve linearly separable problems.</li>
  <li>The LMS algorithm is more powerful than the perceptron learning rule.</li>
  <li>The perceptron rule is guarantedd to converge to a solution that correctly
categorizes the training patterns, the resulting network can be sensitive to
noise, since patterns often lie close to the decision boundaries.</li>
  <li>The LMS algorithm minimizes mean square error, and therefore tries to move
the decision boundaries as far from the training patterns as possible.</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/">implement backprogagation from scratch by python</a></li>
  <li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">how the backpropagation algorithm works</a></li>
  <li><a href="https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a">All you need to know about neural network and back propagation</a></li>
  <li><a href="https://github.com/slip-slap/AI/tree/master/bp">code from scratch in github</a></li>
</ol>
:ET
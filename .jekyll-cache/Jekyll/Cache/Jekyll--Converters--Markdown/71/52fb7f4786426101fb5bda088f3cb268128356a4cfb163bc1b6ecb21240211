I"É
<h3 id="1-definition">1. Definition</h3>
<ol>
  <li>
    <p>For $ p(X | \theta) $, fisher information is to answer the question: <strong>How
useful is the random variable X in determining the unkown parameter
$\theta$</strong></p>
  </li>
  <li>the fisher information is a frequentist concept</li>
  <li><strong>The fisher information is a way of measuring the amount of information that
observable variable X carries about an unknown parameter $\theta$ upoon which
the probability of X depends.</strong></li>
</ol>

<h4 id="terminology-fisher-information">Terminology: Fisher Information</h4>
<p>The Fisher Information is a mearsure for the amount of the information that is
expected within the prototypical trial X about the parameter of interest
$\theta$. It is defined as the variance of the so-called score function,i.e.,the
derivative of the log-likelihood function with respect to the parameter.</p>

<p>$
I(\theta)=-E\left(\frac{\mathrm{d}^{2}}{\mathrm{d} \theta^{2}} \log f(X | \theta)\right)=-\int_{
    \mathcal{X}}\left(\frac{\mathrm{d}^{2}}{\mathrm{d} \theta^{2}} \log f(x | \theta)\right) 
    f(x | \theta) \mathrm{d} x
$</p>

<ul>
  <li><strong>To calculate $I(\theta)$ we keep $\theta$ fixed and take the expectation with
respect to all possible outcomes x</strong></li>
  <li>The above line is very important.</li>
</ul>

<h3 id="2-application">2. Application</h3>

<h4 id="21-frequentist-paradigm-sample-size">2.1 Frequentist Paradigm: Sample Size</h4>

<h4 id="22-bayesian-paradigm-default-parameter-prior">2.2 Bayesian Paradigm: Default Parameter Prior</h4>

<h4 id="23-minimum-description-length-paradigm-model-complexity">2.3 Minimum Description Length Paradigm: Model Complexity</h4>

<p>$ \begin{aligned} \mathrm{AIC} &amp;=-2 \log f(\vec{x} | \hat{\theta})+2 d <br />
\mathrm{BIC} &amp;=-2 \log f(\vec{x} | \hat{\theta})+d \log (n) \ \mathrm{FIA}
&amp;=\underbrace{-\log f(\vec{x} | \hat{\theta})}<em>{\text {Goodness-of-fit
}}+\underbrace{\frac{d}{2} \log \frac{n}{2 \pi}}</em>{\text {Dimensionality
}}+\underbrace{\log \left(\int_{\Theta} \sqrt{\operatorname{det} I(\theta)}
\mathrm{d} \theta\right)}_{\text {Geometric complexity }} \end{aligned} $</p>

<h4 id="galois">Galois</h4>
<ol>
  <li>why we study fisher information, because we want to use likelihood function,
especially, <strong>maximum likelihood estimator(MLE)</strong>, so we study the shape of
the likelihood function</li>
  <li>we want to the likelihood function function more peaked, more cocentrated, so
we study the shape, we come up fisher information.</li>
  <li><strong>Isnâ€™t fisher information is another dual problem ? assuming the probability
model is known, and the oiginal problem is: given the theta, calculate the
probability of some events; and the dual problem is, given all the events,
calulate the statistics in the parameter space.</strong></li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://www.quora.com/What-is-an-intuitive-explanation-of-Fisher-information">Intuitive explanation of Fisher Information</a></li>
  <li><a href="https://www.zhihu.com/question/26561604?sort=created">çŸ¥ä¹Žfisher information</a></li>
  <li><a href="http://www.alexander-ly.com/wp-content/uploads/2014/09/LyEtAlTutorial.pdf">Tutorial on Fisher Information</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Fisher_information">fisher information wiki</a></li>
  <li><a href="https://people.missouristate.edu/songfengzheng/Teaching/MTH541/Lecture%20notes/Fisher_info.pdf">fisher
Information</a></li>
</ol>
:ET
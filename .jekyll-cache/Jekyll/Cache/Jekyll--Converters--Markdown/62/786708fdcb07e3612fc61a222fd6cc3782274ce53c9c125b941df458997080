I"º
<h4 id="what-is-the-crap-of-sampling-distribution">What is the crap of Sampling Distribution?</h4>
<ol>
  <li>Suppose $X_1,X_2,\cdots,X_n$ is an identically and independently distributed
sequence, i.e.,$X_1,X_2,\cdots,X_n$ is a sample from some distribution, and
we are interested in the distribution of a <strong>new random variable</strong> $Y=h(X_1,X_2,\cdots,X_n)$ for
some function h.</li>
  <li>In particular, we might want to compute the distribution function of Y or
perhaps its mean and variance. The distribution of Y is sometimes referred to
as its <strong>sampling distribution</strong></li>
  <li>As Y is based on a sample form some underlying distribution.</li>
</ol>

<h4 id="convergence">Convergence</h4>
<ol>
  <li>Convergence in probability:
A sequence of rvâ€™s $Z_1,Z_2,\cdots$ converges in probability to a rv Z if $ \lim <em>{n \rightarrow
\infty} \operatorname{Pr}\left{\left|Z</em>{n}-Z\right|&gt;\epsilon\right}=0 $ for every $\epsilon &gt; 0$</li>
  <li>
    <p>Convergence in distribution:
A sequence of rvâ€™s $Z_1,Z_2,\cdots$ converges in distribution to a random variable $Z$ if $ \lim <em>{n
\rightarrow \infty} \mathrm{F}</em>{Z_{n}}(z)=\mathrm{F}_{Z}(z) $
at each $z$ for which $F_Z(z)$ is continuous.</p>
  </li>
  <li>
    <p>Convergence in mean square(MS): A sequence of rvâ€™s $Z_1,Z_2,\cdots$ converges
in mean square(MS) to a rv Z if $ \left.\lim <em>{n \rightarrow \infty}
\mathrm{E}\left[\left(Z</em>{n}-Z\right)^{2}\right]\right} 0 $</p>
  </li>
  <li>Convergence with probability 1(WP1):
Let $Z_1,Z_2,\cdot$ be a sequence of rvâ€™s in a sample space $\Omega$ and let Z
be another rv in $\Omega$. Then ${ Z_n; n\geq 1}$ is defined to converge to Z
with probability(WP1) if 
$
\operatorname{Pr}\left{\omega \in \Omega: \lim <em>{n \rightarrow \infty}
Z</em>{n}(\omega)=Z(\omega)\right}=1
$</li>
</ol>

<h4 id="basic-operator">Basic Operator</h4>
<ol>
  <li>Expectation E(X)
$
\mathrm{E}[\mathrm{a} \mathrm{x}+\mathrm{b}]=\mathrm{aE}[\mathrm{X}]+\mathrm{b}
$</li>
</ol>

<p><strong>you can treat expection is a linear operator</strong>
<strong>you can treat expection is a linear operator</strong>
<strong>you can treat expection is a linear operator</strong>
<strong>Just unbelievable !!!</strong></p>
<ol>
  <li>Variation D(X)
$
D(\mathrm{X})=\mathrm{E}\left[(\mathrm{X}-\mathrm{m})^{2}\right]
$ <br />
$
D(X)=E(X^2)-m^2 
$
where m is the expected value E(X)</li>
</ol>

<p>$
\operatorname{D}[a X+b]=a^{2} \operatorname{D}(X)
$</p>

<ol>
  <li>Proof
$
\begin{array}{l}{=\mathrm{E}\left[\mathrm{a}^{2} \mathrm{X}^{2}+2 \mathrm{ab}
\mathrm{X}+\mathrm{b}^{2}\right]-(\mathrm{aE}(\mathrm{X})+\mathrm{b})^{2}} <br />
{=\mathrm{a}^{2} \mathrm{E}\left(\mathrm{X}^{2}\right)+2 \mathrm{ab}
\mathrm{E}(\mathrm{x})+\mathrm{b}^{2}-\mathrm{a}^{2}
\mathrm{E}^{2}(\mathrm{x})-2 \mathrm{abE}(\mathrm{x})-\mathrm{b}^{2}} <br />
{=\mathrm{a}^{2} \mathrm{E}\left(\mathrm{X}^{2}\right)-\mathrm{a}^{2}
\mathrm{E}^{2}(\mathrm{x})=\mathrm{a}^{2}
\operatorname{Var}(\mathrm{X})}\end{array}
$</li>
</ol>

<h4 id="example-mean-and-variance-of-sample">Example: mean and variance of sample</h4>
<p>$
E(\bar{X})=E\left(\frac{X_{1}+X_{2}+\cdots+X_{n}}{n}\right)
$
<br />
$
E(\bar{X})=\frac{1}{n}\left[E\left(X_{1}\right)+E\left(X_{2}\right)+\cdots+E\left(X_{n}\right)\right]
$
<br />
$
\operatorname{D}(\bar{X})=\operatorname{D}\left(\frac{X_{1}+X_{2}+\cdots+X_{n}}{n}\right)
$
<br /></p>

<p>$
\operatorname{D}(\bar{X})=\operatorname{D}\left(\frac{1}{n}
X_{1}+\frac{1}{n} X_{2}+\cdots+\frac{1}{n} X_{n}\right)
$
<br /></p>

<h4 id="example-2">Example 2:</h4>
<p>$S_n-n\bar{X}$</p>

<h4 id="introduction">Introduction</h4>
<p>Inequalities, or bounds, play an unusually large role in probability. Part of
the reason is their frequent use in limit theorems and part is an inherent
imprecision in probability application.</p>

<h4 id="markov-inequality">Markov Inequality</h4>
<p>If Y is a non-negative rv with an expectation E[Y], then for any real $y&gt;0$</p>

<h4 id="chebyshev-inequality">Chebyshev Inequality</h4>
<p>If Z has a mean $E[Z]=\bar{Z}$ and a variance ,$\sigma_{Z}^2$,then for and</p>

<p>Proof: Let $Y=(Z-\bar{Z})^2$. Then E[Y]=$\sigma_{Z}^2$ and for any $y&gt;0$</p>

<h4 id="chernoff-bound">Chernoff bound</h4>
<p>For any $z&gt;0$ and any $r&gt;0$ such that the moment generating function</p>

<h4 id="galois">Galois</h4>
<ol>
  <li>Sometimes proof by image is more intuitively, and simply</li>
  <li>The most important thing is inequality and CDF function, you need to get used
to them.</li>
  <li>Treat $E$ as an linear operator, this idea is awesome, $D$ is also a
operator, but not linear.</li>
  <li>Every subject has its theory, like in physics, the theory is $F=ma$,this is
the theory part.</li>
  <li>CDF function play a very very important role in this subject.</li>
  <li>Sampling distribution, what does this mean ?  figure it out !!!!!!!</li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://revisionmaths.com/advanced-level-maths-revision/statistics/expectation-and-variance">expectation and variance</a></li>
  <li><a href="https://newonlinecourses.science.psu.edu/stat414/node/167/">mean and variance of sample
mean</a></li>
  <li><a href="https://www.stat.auckland.ac.nz/~fewster/325/notes/ch3.pdf">expectation and variance</a></li>
</ol>

:ET
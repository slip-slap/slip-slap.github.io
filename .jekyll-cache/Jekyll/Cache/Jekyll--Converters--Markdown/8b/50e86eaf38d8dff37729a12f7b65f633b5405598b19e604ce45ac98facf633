I"á
<ul>
  <li>Markov Chain is just a fancy term for a random walk on a graph</li>
  <li>what it says it that for a very long walk, the probability that you end at
some vertex <em>V</em> is independent of where you started</li>
</ul>

<p><img src="/images/Markov-Chain.png" height="" width="" /></p>

<h4 id="definition">definition</h4>
<p>Ergodic Markov Chains: A Markov Chain is ergodic if there is a positive probability to pass from any state to any other states in one step.</p>
:ET
I"}
<h4 id="1-motivation">1. Motivation</h4>
<p>We usually expect $P(\alpha|\beta)$ to be different from $P(\alpha)$. That is,
learning that $\beta$ is true changes our probability over $\alpha$, However, in
some situations equality can occur, so that <br />
$P(\alpha|\beta)=P(\alpha)$. <br />
That is, learning that $\beta$ occurs did not change our probability of $\alpha$</p>

<h5 id="31-conditional-independence-what-is-independent-of-what">3.1 Conditional Independence: what is independent of what?</h5>
<p>Independence is a useful property, it is not often that we encounter two
independent events. A more common situation is when two events are independent
given an additional event.</p>
<ol>
  <li>Conditionally Independent:  We say that an event $\alpha$ is conditionally independent of event $\beta$
given $\gamma$ in P, denoted</li>
  <li>The initial intuition for conditional probability comes from considering
probabilities that are ratios. In the case of ratios, <strong>P(E|F)</strong>, as defined
above, is the fraction of items in <strong>F</strong> that are also in <strong>E</strong>.</li>
</ol>

<h5 id="definition-1">Definition 1:</h5>
<p>A distribution P satisfies$(\alpha \perp \beta)$ if and only if 
\(P(\alpha \cap \beta)=P(\alpha) P(\beta)\)</p>

<h4 id="why-independence-and-conditional-independence-are-so-important-">Why Independence and Conditional Independence are so Important ?</h4>
<ol>
  <li>we can always write the following formula, no matter they are independent or
not. <br />
 $P(x,y| \mathcal{H})=P(x|y,\mathcal{H})p(y|\mathcal{H})$</li>
  <li>if they are independent, it can be denoted in the following way <br />
 $P(x,y| \mathcal{H})=P(x|\mathcal{H})p(y|\mathcal{H})$
    <ul>
      <li>Maybe because we have just two random variables,you can’t realize how
important they are.
        <h4 id="notation">Notation</h4>
      </li>
    </ul>
  </li>
  <li>PMF: If X has only a finite or countable number of possible sample values,
say $x_1,x_2,\cdots$, the probability $Pr{X=x_i}$ of each sample values
$x_i$ is called the probability mass function(PMF) at $x_i$ and denoted by
$p_X(x_i)$; such a random variable is called discrete</li>
  <li>Probability Density: If the distribution function $F_X(x)$ of a rv X has a
derivative at x, the derivative is called the probability density of X at x
and denoted by $f_X(x)$
    <h5 id="remark">Remark</h5>
    <p>Probability Mass function and probability density function are defined based on
cumulative distribution function.</p>
  </li>
</ol>

<h4 id="galois">Galois</h4>
<ol>
  <li>What does the intuitive behind conditional independence? It’s relationship,
which is the essence all mathematics about, the only thing that we care. It
means the connection of the world.</li>
  <li>We also give a way to check this relationship by $ P(\alpha \cap
\beta)=P(\alpha) P(\beta) $. From intuitive to a strict mathematical object,
that’s the beauty of mathematics.</li>
</ol>

<h4 id="4-reference">4. Reference</h4>
<ol>
  <li><a href="https://www.matongxue.com/madocs/910/">beta 分布-马同学</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Beta_distribution">beta distribution-wikipedia</a></li>
</ol>
:ET
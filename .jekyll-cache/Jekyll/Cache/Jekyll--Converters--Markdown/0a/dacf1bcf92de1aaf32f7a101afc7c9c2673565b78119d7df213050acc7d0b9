I"M
<h4 id="idea">Idea</h4>
<p>In statistics, a statistic is sufficient with respect to a statistical model and
its assoiated unknown parameter if “no other statistic that can be calculated
from the same sample provides any additional information as to the the value of
the parameter.”</p>

<p><strong>Fisher-Neyman Factorization theorem</strong>: If the density(or probability function) for a model
factoer as $f_X(x)=h(x)g(\theta, T(x))$, then T is a sufficient statistic.</p>
<ul>
  <li>The density $f$ can be factored into a product such that one factor, $h$, does
not depend on $\theta$ and the other, which does depend on $x$ only through
$T(x)$</li>
  <li>It is easy to see that if $F(t)$ is a one-to-one function and $T$ is a
sufficient statistic, then $F(T)$ is a sufficient statistic. In particular we
can multiply a sufficient statistic by a nonzero constant and get another
sufficient statistic.</li>
</ul>

<h4 id="galois">Galois</h4>
<ol>
  <li>We define sufficient statistics, but how could we identify them? this is
where the <em>factorization theorem</em> comes to play.</li>
</ol>

:ET
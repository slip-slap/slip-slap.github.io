I"P
<h4 id="one-dimensional-gmm">One Dimensional GMM</h4>
<p><img src="/images/AI-model-gmm.png" />
The question is how to understand the coefficient between each $x_i$</p>
<ol>
  <li>suppose a point value is 10, and the coefficient is very small which is
0.02, you can understand this in this way, this contribution of this point is
kind of pull the Gauss distribution to the left, becasue its contribution to
the mean value of Gauss is 0.2. It is kind of drag the distribution to the
left part of the coordinate.</li>
  <li>How to understand the coeffient pops up in the denominator, You can treat it
in this way, if you donâ€™t have the the coefficient, what should be as
denominator.</li>
</ol>

<h4 id="model">model</h4>

<p>we are assuming that these data are Gaussian and we want to maximize the likelihood of 
observing these data.The following equation tells us that a particular point <strong>x</strong> is a
<strong>linear combination of the K Gaussians.</strong></p>

\[\begin{array}{c}{p(x)=\sum_{j=1}^{k} \phi_{j} \mathcal{N}\left(x ; \mu_{j}, \Sigma_{j}\right)} \\ {\sum_{j=1}^{k} \phi_{j}=1}\end{array}\]

<h4 id="expectation-maximization">Expectation-Maximization</h4>

<p>At the Expectation step, we compute a matrix where rows are the data points and the column are Gaussians.
An element at row <strong>i</strong>, column <strong>j</strong> is that the probability that \(x^{i}\) was generated by the Gaussian <strong>j</strong>
The denominator just sums over all values to make <strong>W</strong> a probability</p>

\[W_{j}^{(i)}=\frac{\phi_{j} \mathcal{N}\left(x^{(i)} ; \mu_{j}, \Sigma_{j}\right)}{\sum_{q=1}^{k} \phi_{q} \mathcal{N}\left(x^{(i)} ; \mu_{q}, \Sigma_{q}\right)}\]

<ol>
  <li>The first equation is just the sum of a particular Gaussian <strong>j</strong> divided by the number of points</li>
  <li>In the second equation, we just computing the mean, except we multiply by the probabilities for that cluster</li>
  <li>In the last equation, we are just computing the covariance, except we multiply by the probabilities for that cluster</li>
</ol>

\[\begin{aligned} \phi_{j} &amp;=\frac{1}{N} \sum_{i=1}^{N} W_{j}^{(i)} \\ \mu_{j} &amp;=\frac{\sum_{i=1}^{N} W_{j}^{(i)} x^{(i)}}{\sum_{i=1}^{N} W_{j}^{(i)}} \\ \Sigma_{j} &amp;=\frac{\sum_{i=1}^{N} W_{j}^{(i)}\left(x^{(i)}-\mu_{j}\right)\left(x^{(i)}-\mu_{j}\right)^{T}}{\sum_{i=1}^{N} W_{j}^{(i)}} \end{aligned}\]

<table>
  <thead>
    <tr>
      <th style="text-align: center">GMM</th>
      <th style="text-align: center">Cluster 1</th>
      <th style="text-align: center">Cluster 2</th>
      <th>Cluster 3</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">data 1</td>
      <td style="text-align: center">\(w_{1}^{1}\)</td>
      <td style="text-align: center">\(w_{2}^{1}\)</td>
      <td>\(w_{3}^{1}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">data 2</td>
      <td style="text-align: center">\(w_{1}^{2}\)</td>
      <td style="text-align: center">\(w_{2}^{2}\)</td>
      <td>\(w_{3}^{2}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">data 3</td>
      <td style="text-align: center">\(w_{1}^{3}\)</td>
      <td style="text-align: center">\(w_{2}^{3}\)</td>
      <td>\(w_{3}^{3}\)</td>
    </tr>
    <tr>
      <td style="text-align: center">data 4</td>
      <td style="text-align: center">\(w_{1}^{4}\)</td>
      <td style="text-align: center">\(w_{2}^{4}\)</td>
      <td>\(w_{3}^{4}\)</td>
    </tr>
  </tbody>
</table>

<h4 id="galois">Galois</h4>
<ol>
  <li>If you donâ€™t understand in two dimensional GMM, maybe you should start with
one dimensional GMM.</li>
  <li>There is one question left, how to understand the coefficient when we
calculate the mean, covaiance.</li>
</ol>

<h4 id="reference">Reference</h4>
<p><a href="https://pythonmachinelearning.pro/clustering-with-gaussian-mixture-models/">Clustering with Gaussian Mixture Model</a></p>
:ET
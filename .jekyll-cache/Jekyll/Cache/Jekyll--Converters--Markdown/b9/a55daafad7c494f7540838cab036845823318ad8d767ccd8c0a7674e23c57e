I"Ö
<h4 id="introduction">Introduction</h4>
<p>At the heart of the theory of inference is the concept of the statistical
model$f_{\theta}: \theta \in \omega$ that describes the statisticianâ€™s
uncertainty about how the observed data were produced. 
<strong>all
   uncertainties need to be described by probabilities</strong>, then the prescription of
   a model alone is incomplete.</p>

<p><img src="/images/math-apply-probability-and-statistics-bayesian-inference.png" /></p>

<ul>
  <li>The spread of the posterior distribution gives some idea of the precision on
any probability statements we make about $\theta$</li>
  <li>Note how much information the data have added, as reflected in the graphs of
the prior and posterior densities.</li>
</ul>

<h4 id="2-basic-rule">2 Basic rule</h4>
<ol>
  <li>
\[P(x)=\sum_{y} P(x, y)\]
  </li>
  <li>
\[P(x, y)=P(y \mid x) P(x)\]
  </li>
  <li>
\[P(y \mid x)=\frac{P(x \mid y) P(y)}{P(x)}\]
  </li>
  <li>
\[P(x)=\sum_{y} P(x \mid y) P(y)\]
  </li>
</ol>

<h4 id="galois">Galois</h4>
<ol>
  <li>beyond the model and the data, prior belief is introduced into inference,
thatâ€™s bayesian inference.</li>
  <li>Why we called this posterior inference, because we have a prior inference.</li>
</ol>

:ET
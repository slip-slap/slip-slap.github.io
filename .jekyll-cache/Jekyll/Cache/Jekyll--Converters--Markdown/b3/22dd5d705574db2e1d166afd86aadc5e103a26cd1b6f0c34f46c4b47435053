I"·
<ul>
  <li>Markov Chain is just a fancy term for a random walk on a graph</li>
  <li>what it says it that for a very long walk, the probability that you end at
some vertex <em>V</em> is independent of where you started</li>
</ul>

<p><img src="/images/Markov-Chain.png" height="" width="" /></p>

<h4 id="definition">definition</h4>
<p>Ergodic Markov Chains: A Markov Chain is ergodic if there is a positive probability to pass from any state to any other states in one step.</p>

<h4 id="galois">Galois</h4>
<ol>
  <li>The role of Markov Chain in stochastic process is similar to the Newtonâ€™s Law in Physics.</li>
  <li>Both of them predict the future based on the current state.</li>
</ol>
:ET
I"§
<h3 id="brief-description-for-ensemble-bagging-and-boosting">Brief Description for Ensemble, Bagging and Boosting</h3>

<p>An ensemble is just a collection of predictors which come together(e.g. mean of all predictions)
to give a final prediction. The reason we use ensembles is that many different predictors trying to
predict same target variable will perform a better job than any single predictor alone. <br /></p>

<p>Ensembling techiques are further classified into Bagging and Boosting.</p>

<ol>
  <li>
    <p>Bagging is a simple ensembling techique in which we build many independent predictors/models/learners
and combine them using some model averaging techiques.</p>
  </li>
  <li>
    <p>Boosting is an ensemble techique in which the predictors are not made independently, but sequentially.</p>
  </li>
</ol>

<h3 id="what-is-the-crap-of-gradient-boosting">what is the crap of gradient boosting?</h3>

<p>Gradient Boosting: use additive modeling to gradually nudge an approximate mode towards a really good model,
by adding simple submodels to a composite model. <br />
This is crap, what is tha crap of <strong>additive modeling ?</strong>
we have to answer the question,</p>

<h4 id="what-is-the-crap-of-additive-modeling-2">what is the crap of additive modeling &lt;2&gt;</h4>

<p>Defition: Adding up a bunch of subfunctions to create a composite function that models some data points 
is then called additive modeling.</p>

<h4 id="an-introduction-to-boosted-regression">An introduction to boosted regression</h4>

<p><strong>Boosting</strong> is a loosely-defined strategy that combines multiple simple models into a single composite model.
The idea is that, as we introduce more simple models, the overall model becomes a stronger and stronger predictor.
In boosting terminology, the simple models are called weak models or weak learners. <br /></p>

<p>In practice, we choose the number of stages, M, as a hyperparameter of the overall model. Allowing M to grow arbitrarily
increases the risk of overfitting.</p>

\[F_{m}(\mathbf{x})=F_{m-1}(\mathbf{x})+f_{m}(\mathbf{x})\]

<p>Boosting itself does not specify how to choose the weak learners. Boosting does not even specify the form of the 
\(f_{m}(\mathrm{x})\) models, but the form of the weak model dictates the form of the meta-model. <br />
For example, if all weak models are linear models, then the resulting meta-model is a simple linear model. <br />
If we use tiny regression trees  as the weak models, the result is a forest of trees whose predictions are added together.</p>

<h4 id="the-intuition-behind-gradient-boosting">The intuition behind gradient boosting</h4>
<p>It might be helpful to think of this boosting approach as a golfer initially whacking a golf ball towards the hole at y
but only getting as far as \(f_{0}(\mathbf{x})\).</p>

<p>The golfer then repeatedly taps the ball more softly, working the ball towards the hole, after reassessing direction and distance
to the hole at each stage. The following diagram illustrates 5 strokes getting to the hole, y, including two strokes,
\(\Delta_{2} \text { and } \Delta_{3}\), that overshoot the hole</p>

<p><img src="/images/GBM-golfer.png" height="" width="" /></p>

<h4 id="the-difference-between-random-forest-and-boosted-trees">The Difference Between Random Forest and Boosted Trees</h4>
<p>Common: Random Forest and Boosted Trees are really the same models;
Difference: The difference is how we train them</p>

<h3 id="reference">Reference</h3>

<ol>
  <li><a href="https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d">Gradient Boosting from scratch</a></li>
  <li><a href="https://explained.ai/gradient-boosting/L2-loss.html">An introduction to additive modeling</a></li>
</ol>
:ET
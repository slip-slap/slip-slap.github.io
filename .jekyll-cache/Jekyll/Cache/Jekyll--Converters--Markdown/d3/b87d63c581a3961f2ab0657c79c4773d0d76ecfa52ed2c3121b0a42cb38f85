I"”
<h3 id="a-neuron">a neuron</h3>
<p>the weights bias(b) namely is the threshold
<img src="/images/neuron.png" height="" width="" /></p>

<h3 id="the-architecture-configuration-of-nn">The architecture configuration of N.N.</h3>

<h4 id="output-layer">Output layer</h4>

<ol>
  <li>If N.N. is a regressor, then the output has a single node.</li>
  <li>If N.N. is a classifer, t</li>
</ol>

\[\begin{align*}
  &amp; \phi(x,y) = \phi \left(\sum_{i=1}^n x_ie_i, \sum_{j=1}^n y_je_j \right)
    = \sum_{i=1}^n \sum_{j=1}^n x_i y_j \phi(e_i, e_j) = \\
      &amp; (x_1, \ldots, x_n) \left( \begin{array}{ccc}
            \phi(e_1, e_1) &amp; \cdots &amp; \phi(e_1, e_n) \\
                  \vdots &amp; \ddots &amp; \vdots \\
                        \phi(e_n, e_1) &amp; \cdots &amp; \phi(e_n, e_n)
                            \end{array} \right)
        \left( \begin{array}{c}
              y_1 \\
                    \vdots \\
                          y_n
                              \end{array} \right)
        \end{align*}\]

<p>the formulation \(p(y \vert x)\) is used to mean ‚Äúthe probability of \(y\) given \(x\) ‚Äú
the cost function</p>

<p>If this is only one node in the output layer of neural network
\(\begin{equation}
Cost(h_{\theta},y) = -y\log( h_{\theta}(x)) - (1-y)\log(1-h_{\theta}(x)) 
\end{equation}\)</p>

<p>if we generalize this for multiple output nodes, and we get:</p>

\[\begin{equation*}
J(\Theta) = -\frac{1}{m}\sum_{i=1}^{m}\sum_{k=1}^{K}[y_k^{(i)}\log((h_{\Theta}^{(x^(i)}))_k)+(1-y_k^{(i)})\log(1-(h_{\Theta}^{(x^(i)}))_k)]+\frac{\lambda}{2m}\sum_{i=1}{L-1}
\end{equation*}\]

<h3 id="ÂèÇËÄÉÈìæÊé•">ÂèÇËÄÉÈìæÊé•</h3>
<ol>
  <li><a href="https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/">implement backprogagation from scratch by python</a></li>
  <li><a href="http://neuralnetworksanddeeplearning.com/chap2.html">how the backpropagation algorithm works</a></li>
  <li><a href="https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a">All you need to know about neural network and back propagation</a>
4.<a href="https://github.com/slip-slap/AI/tree/master/bp">code from scratch in github</a></li>
</ol>
:ET
I"¥
<h4 id="1-introduction">1. Introduction</h4>
<p>At the heart of the theory of inference is the concept of the statistical
model$f_{\theta}: \theta \in \omega$ that describes the statisticianâ€™s
uncertainty about how the observed data were produced. 
<strong>all
   uncertainties need to be described by probabilities</strong>, then the prescription of
   a model alone is incomplete.</p>

<p><img src="/images/math-apply-probability-and-statistics-bayesian-inference.png" /></p>

<ul>
  <li>The spread of the posterior distribution gives some idea of the precision on
any probability statements we make about $\theta$</li>
  <li>Note how much information the data have added, as reflected in the graphs of
the prior and posterior densities.</li>
</ul>

<h4 id="2-basic-rule">2. Basic rule</h4>

\[P(\text { hypothesis } \mid \text { data })=\frac{P(\text { data } \mid \text { hypothesis }) P(\text { hypothesis })}{P(\text { data })}\]

<ol>
  <li>sum rule: \(P(x)=\sum_{y} P(x, y)\)</li>
  <li>product rule: \(P(x, y)=P(y \mid x) P(x)\)</li>
  <li>combine product rule and sum rule: \(P(x)=\sum_{y} P(x \mid y) P(y)\)</li>
</ol>

<ul>
  <li>the $P(\text { data } \mid \text { hypothesis })$ above is refered as
likelihood, not probability, since the $data$ here is observation.</li>
  <li>The sum of $P(\text { data } \mid \text { hypothesis })$ over the hypotheses
maybe not equal to 1. Therefore we canâ€™t refer to this entry as probability.</li>
</ul>

<h4 id="galois">Galois</h4>
<ol>
  <li>beyond the model and the data, prior belief is introduced into inference,
thatâ€™s bayesian inference.</li>
  <li>Why we called this posterior inference, because we have a prior inference.</li>
</ol>

:ET
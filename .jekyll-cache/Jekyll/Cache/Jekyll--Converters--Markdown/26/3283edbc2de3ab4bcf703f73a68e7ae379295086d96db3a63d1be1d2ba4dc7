I"›
<h4 id="inspiration">Inspiration</h4>
<ol>
  <li>A basic idea in information theory is that information can be treated very
much like a physical quantity, such as mass or quantity.</li>
</ol>

<ul>
  <li>units: information is measured in bits, and one bit of information allows you
to choose between two equally probable</li>
</ul>

<h4 id="information">Information</h4>

<p>$\textbf{Shannon information} = log \frac{1}{p(x_h)} \textbf{bits}$</p>

<h4 id="bits-are-not-binary-digits">Bits Are Not Binary Digits</h4>
<p>The word bit is derived from binary digit, but a bit and a binary digit are
fundamentally different types of quantities.</p>
<ul>
  <li>A binary digit is the value of a binary variable</li>
  <li>a bit is an amount of information</li>
</ul>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="http://users.fred.net/tds/lab/information.is.not.uncertainty.html">information and
entropy</a></li>
</ol>

:ET
I"Ù
<h4 id="em-algorithm">EM algorithm</h4>

<p>EM is an algorithm for maximum the likelihood function</p>

<p><strong>å¹¿ä¹‰EMç®—æ³•ï¼ŒEæ­¥éª¤æ˜¯å›ºå®šå‚æ•°ä¼˜åŒ–éšåˆ†å¸ƒï¼ŒMæ­¥éª¤æ˜¯å›ºå®šéšåˆ†å¸ƒä¼˜åŒ–å‚æ•°,å¯ä»¥ç”¨GMMç®—ä½œä¸ºç±»æ¯”</strong></p>

<h4 id="jensens-inequality">Jensenâ€™s inequality</h4>
<p>Jensenâ€™s inequality generalizes the statement that the secant line of a convex lies above the graph of the function.
which in Jensenâ€™s inequality for two points. The Jensenâ€™s inequality is(for \(t\in[0,1]\)):</p>

\[f\left(t x_{1}+(1-t) x_{2}\right) \leq t f\left(x_{1}\right)+(1-t) f\left(x_{2}\right)\]

<p>in the context of probability, it is generally stated in the following form, if X is a random variable and 
\(\varphi\) is a convex function</p>

\[\varphi(\mathrm{E}[X]) \leq \mathrm{E}[\varphi(X)]\]

<p><img src="/images/convex.png" height="" width="" /></p>

<h4 id="galois">Galois</h4>
<ol>
  <li>EM algorithm, what does E and M, respectively, stand for?</li>
  <li>They denote the expectation and maximum of the parameter theta that we are
looking for, nothing about the latent variable, itâ€™s just a tool to make the
topic eazier to understand.
    <ul>
      <li>suppose we have a mixed guassian model, they are $N_1(\mu^1,\sigma^1)$ and $N_2(\mu^2,\sigma^2)$</li>
    </ul>
  </li>
</ol>

<h4 id="reference">Reference</h4>
<ol>
  <li><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm">Expectation and Maximization</a></li>
</ol>

:ET
I"Î
<h4 id="inspiration">Inspiration</h4>
<ol>
  <li>The key strategy for obtaining good generalization is to find the simplest
model that explains the data. This is a variation of a principle called
Ockhamâ€™s razor, which is named after the English logician William of Ockham.</li>
  <li>
    <p>The idea is that the more complexity you have in your model, the greater the
possibility for errors.</p>
  </li>
  <li>In terms of neural networks, the simplest model is the one that contains the
smallest number of free parameters(weights and biases), or, equivalently, the
smallest number of neurons. To find a network that generalizes well, we need
to find the simplest network that fits the data.</li>
</ol>

<h4 id="method">Method</h4>
<ol>
  <li>Growing: starts with no neurons in the network and then add neurons until the
performance is adequate.</li>
  <li>pruning: start with large networks, which likely overfit, and then remove
neurons one at a time until the performance degrades significantly.</li>
  <li>global searches: such as genetic algorithms, search the space of all possible
network architectures to locate the simplest model that explains the data.</li>
  <li>regularization</li>
  <li>early stopping</li>
</ol>

:ET